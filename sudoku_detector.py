# -*- coding: utf-8 -*-
"""sudoku_detector.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1WV08oBuRLOibzbsG1AXPVD-uKe5GpkY0
sudoku_detector(img):
Transforms on image:
1.cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) converts the image to grayscale
2.Adding GaussianBlur
3.Image thresholding is an important intermediary step for image processing pipelines. Thresholding can help us to remove lighter or darker regions and contours of images
"""
import cv2
import imutils
import numpy as np
from imutils.perspective import four_point_transform
from google.colab.patches import cv2_imshow
import operator


def sudoku_detector(img,show = False, dilate = True):
  image_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  
  image_blur = cv2.GaussianBlur(image_gray,(5,5),3)
  
  image_threshold = cv2.adaptiveThreshold(image_blur, 255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)

  
  image_processed = image_threshold
  if dilate == True:
    kernel = np.array([[0., 1., 0.], [1., 1., 1.], [0., 1., 0.]],np.uint8)
    image_processed = cv2.dilate(image_threshold, kernel)

  
  original_pic = img.copy()
  if show == True:
        cv2_imshow(image_gray)
        cv2_imshow(image_blur)
        cv2_imshow(image_threshold)
        
        cv2_imshow(image_processed)
          
  return(image_processed)

def find_boundary(original_img,img,show = True):
  contours = cv2.findContours(img.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)

  contours = imutils.grab_contours(contours)
  original_pic = original_img.copy()
  from google.colab.patches import cv2_imshow
  for i,contour in enumerate(contours):
    approx_box = cv2.approxPolyDP(contour, 0.01*cv2.arcLength(contour,True),True)
    if len(approx_box)==4:
      X,Y,W,H = cv2.boundingRect(approx_box)
      
      if (H>3 and W>3):
        cv2.drawContours(original_pic, [approx_box], 0, (0,0,255),5)
        
        cv2_imshow(original_pic)
        cv2_imshow(img)
        top_left , _ = min(enumerate([tl[0][0] + tl[0][1] for tl in approx_box]), key= operator.itemgetter(1))
        top_right , _ = max(enumerate([tr[0][0] - tr[0][1] for tr in approx_box]), key= operator.itemgetter(1))
        bottom_left , _ = min(enumerate([bl[0][0] - bl[0][1] for bl in approx_box]), key= operator.itemgetter(1))
        bottom_right , _ = max(enumerate([br[0][0] + br[0][1] for br in approx_box]), key= operator.itemgetter(1))
  
        return([approx_box[top_left][0],approx_box[top_right][0],approx_box[bottom_left][0],approx_box[bottom_right][0]])
        


def euclidean_distance(pt1, pt2): 
    x = pt2[0] - pt1[0] 
    y = pt2[1] - pt1[1] 
    return np.sqrt((x ** 2) + (y ** 2))
    
def crop_and_warp(img,box_array):
  top_left,top_right,bottom_left,bottom_right = box_array
   
  #breadth and length of new image
	#Maximum distance between bottom-right and bottom-left x-coordinates or the top-right and top-left x-coordinates
  breadth_1 = euclidean_distance(bottom_left,bottom_right)
  breadth_2 = euclidean_distance(bottom_left,bottom_right)
  max_breadth = max(int(breadth_1),int(breadth_2))
  #Maximum distance between the top-right and bottom-right y-coordinates or the top-left and bottom-left y-coordinates
	
  length_1 = euclidean_distance(top_right,bottom_right)
  length_2 = euclidean_distance(top_left,bottom_left)
  max_length = max(int(length_1),int(length_2))
  #construct the set of destination points to obtain a "top-down view",  of the image, again specifying points 
  #in the top-left, top-right, bottom-right, and bottom-left order
  destination_pts = np.array([[0,0],[max_breadth-1,0],[max_breadth-1,max_length-1],[0,max_length-1]],dtype = "float32")
  # compute the perspective transform matrix and then apply it
  image_perspective = cv2.getPerspectiveTransform(box_array,destination_pts)
  image_warped = cv2.warpPerspective(img,image_perspective,(max_breadth,max_length))
  cv2_imshow(image_warped)
  return image_warped
	
  


from skimage.segmentation import clear_border
def digit_extraction(img,position,img_size,grid_size,extracted_img_size,show = False):
  image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  block_size_h = img_size[0]//grid_size[0]
  block_size_w = img_size[1]//grid_size[1]
  h_i = block_size_h*(position[0])
  h_f = h_i + block_size_h
  w_i = block_size_w*(position[1])
  w_f = w_i + block_size_w
  digit = image[h_i:h_f,w_i:w_f]
  digit = cv2.resize(digit,extracted_img_size)
  #digit_gray = cv2.cvtColor(np.array(digit, dtype=np.uint8), cv2.COLOR_BGR2GRAY)
  digit_threshold = cv2.adaptiveThreshold(digit , 255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,27,2)
      
      
  if show == True:
    cv2_imshow(digit)
    cv2_imshow(digit_threshold)

  return(digit_threshold)
  
